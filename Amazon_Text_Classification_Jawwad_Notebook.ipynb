{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "DxuOL67I7zX6",
    "outputId": "3004e65f-0d7e-4463-e772-6b64c57b8e99"
   },
   "outputs": [],
   "source": [
    "#Use following code to install multilingual Amazon Corpus as ARROW file\n",
    "#Pip install datasets if using Pip\n",
    "#conda install datasets if using Anaconda\n",
    "#from datasets import load_dataset\n",
    "#dataset=load_dataset('amazon_reviews_multi')\n",
    "\n",
    "#creating Neural Network using .csv of Amazon Review Corpus\n",
    "import json\n",
    "from numpy.core.numerictypes import _construct_lookups \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from os import chdir,getcwd,path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Text_Classification():\n",
    "    #set values for Tokenizer and Pad_Sequences\n",
    "    embedding_dim=2\n",
    "    max_length=100\n",
    "    trunc_type=\"post\"\n",
    "    padding_type=\"post\"\n",
    "    oov_tok=\"oov\"\n",
    "    training_size=20000\n",
    "\n",
    "    corpus=[]\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "\n",
    "    num_sentences=0\n",
    "\n",
    "    #Opens .csv values and creates a list of Corpus\n",
    "    with open(\"Documents/GitHub/Fellowship_AI/amazon_review_full_csv/train.csv\",encoding=\"utf-8\") as csvfile:\n",
    "        reader=csv.reader(csvfile,delimiter=',')\n",
    "        for row in reader:\n",
    "            list_item=[]\n",
    "            \n",
    "            #appends review\n",
    "            list_item.append(row[2])\n",
    "\n",
    "            #appends review score based on row value[0]\n",
    "            this_label=row[0]\n",
    "            if this_label=='1':\n",
    "                list_item.append(1)\n",
    "            elif this_label=='2':\n",
    "                list_item.append(2)\n",
    "            elif this_label=='3':\n",
    "                list_item.append(3)\n",
    "            elif this_label=='4':\n",
    "                list_item.append(4)\n",
    "            else:\n",
    "                list_item.append(5)\n",
    "\n",
    "            num_sentences=num_sentences+1\n",
    "            corpus.append(list_item)\n",
    "\n",
    "    #Test code to check if .csv file was read\n",
    "    #print(num_sentences)\n",
    "    #print(len(corpus))\n",
    "    #print(corpus[30])\n",
    "\n",
    "    #Randomizes the reviews\n",
    "    random.shuffle(corpus)\n",
    "\n",
    "\n",
    "\n",
    "    #seperates the reviews and scores into labels and sentences\n",
    "    for x in range(training_size):\n",
    "        sentences.append(corpus[x][0])\n",
    "        labels.append(corpus[x][1])\n",
    "\n",
    "    #print(sentences)\n",
    "    print(labels)\n",
    "\n",
    "    #tokenizes values and processes input data\n",
    "    tokenizer=Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    word_index=tokenizer.word_index\n",
    "    vocab_size=len(word_index)\n",
    "\n",
    "    sequences=tokenizer.texts_to_sequences(sentences)\n",
    "    padded=pad_sequences(sequences,maxlen=max_length,padding=padding_type,truncating=trunc_type)\n",
    "\n",
    "\n",
    "    #creates training and testing sequences\n",
    "    training_sequences = padded[0:training_size]\n",
    "    test_sequences = padded[training_size:]\n",
    "    training_labels = labels[0:training_size]\n",
    "    test_labels = labels[training_size:]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation=keras.layers.LeakyReLU(alpha=0.001)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "        tf.keras.layers.Conv1D(64, 5, activation=keras.layers.LeakyReLU(alpha=0.001)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "\n",
    "    training_padded = np.array(training_sequences)\n",
    "    training_labels = np.array(training_labels)\n",
    "    testing_padded = np.array(test_sequences)\n",
    "    testing_labels = np.array(test_labels)\n",
    "\n",
    "\n",
    "    history = model.fit(training_padded, training_labels, epochs=100)\n",
    "\n",
    "    #check structure\n",
    "    #model.summary()\n",
    "\n",
    "    #plot results using Matplotlib\n",
    "    acc=history.history[\"acc\"]\n",
    "    epoch=range(len(acc))\n",
    "\n",
    "    plt.plot(epoch,acc,\"r\",label=\"training\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
